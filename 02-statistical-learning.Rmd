
# Statistical Learning

```{r include = FALSE}
# Options
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(error = TRUE)

# Packages
library(tidyverse)
library(ISLR)
library(readr)
library(ggplot2)

# get Advertising and Income1 data from ISLR Site
if(!file.exists("data/Advertising.csv")){
    download.file("https://www.statlearning.com/s/Advertising.csv", 
                  destfile = "data/Advertising.csv")
} 
Advertising <- read_csv("data/Advertising.csv") %>% select(-X1)

if(!file.exists("data/Income1.csv")){
    download.file("https://www.statlearning.com/s/Income1.csv", 
                  destfile = "data/Income1.csv")
} 
Income1 <- read_csv("data/Income1.csv") %>% select(-X1)
# Auto data
if(!file.exists("data/Auto.data")){
    download.file("https://www.statlearning.com/s/Auto.data", 
                  destfile = "data/Auto.data")
} 

```

## 2.1 What Is Statistical Learning?

Motivating example:

> Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. ... our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.

```{r}
glimpse(Advertising)
```

```{r}
Advertising %>%
    ggplot(mapping = aes(x = TV, y = sales)) +
    geom_point(alpha = 0.25, shape = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)
Advertising %>%
    ggplot(mapping = aes(x = radio, y = sales)) +
    geom_point(alpha = 0.25, shape = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)
Advertising %>%
    ggplot(mapping = aes(x = newspaper, y = sales)) +
    geom_point(alpha = 0.25, shape = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)

```

**Input Variables**: These are the variables we know and can use to build our model. Also known as *predictors*, *independent variables*, or *features*. Denoted using the symbol $X_n$.

**Output Variable**: This is the variable we are trying to predict with the model. Also known as a *response*, or *dependent variable*. Typically denoted as $Y$.

More generally: $Y = f(X) + \epsilon$

Where $Y$ is the quantitative response and $f$ is a function of $X_1, ..., X_p$ (of $p$ different predictors) and $\epsilon$ is some random **error term**.

Assumptions:

-   $f$ is **systematic** in its relationship to $Y$
-   $\epsilon$ is independent of $X$
-   $\epsilon$ has mean zero

Another example: Income and education may appear related, but the exact relationship is unknown. Note that some of the observations are above the linear interpolated line, while some are below it. The difference is $\epsilon$

```{r}
Income1 %>%
    ggplot(mapping = aes(x = Education, y = Income)) +
    geom_point(color = "red") + 
        geom_smooth(formula = y~x, method = "lm", se = FALSE) +
    theme_bw() 

```

### 2.1.1 Why Estimate f?

There are two main reasons to estimate $f$:

-   Prediction

-   Inference

#### Prediction

Consider: $\hat{Y} = \hat{f}(X)$

If $X$ is known, we can predict $\hat{Y}$ by this equation. Don't be too concerned with the exact functional form of $\hat{f}$, as long as it yields accurate predictions of $Y$.

The accuracy of $\hat{Y}$ depends on two quantities:

-   **Reducible error**: This is error that comes with the model. We can potentially address this error by improving the accuracy of the model.

-   **Irreducible error**: This is error introduced to the model, because $\epsilon$, by definition, cannot be explained by $X$

**Why is irreducible error larger than zero?** Consider the estimate $\hat{f}$ and a prediction $\hat{Y} = \hat{f}(X)$. Let $\hat{f}$ and $X$ be fixed. Then:

$E(Y - Y^2) = E[f(X) + \epsilon - \hat{f}(X)]^2$

$= [f(X) - \hat{f}(X)]^2 + Var(\epsilon)$

Where $E(Y - Y^2)$ is the **expected value** of the squared difference between the predicted and actual value of $Y$, and $Var(X)$ is the **variance** associated with the error term $\epsilon$.

#### Inference:

When used for inference, the aim is not to use estimate $f$ for predictions, but rather to understand how some response $Y$ is affected by the changes in $X_1, ..., X_p$.

-   **Which predictors are associated with the response?**: Identifying the **important** predictors is the aim here.\
-   **What is the relationship between the response and each predictor?**: This can be positive, negative, or depend on the values of other predictors, depending on how complicated the model is.\
-   **Can the relationship between** $Y$ and each predictor be summarized using a linear equation?

Examples:

**Prediction**: A Company using a model to identify target customers for a direct-marketing campaign. The company is not interested in the model, they just want a function form that will help them.

**Inference**: Modeling customer purchases of specific brands of products. The model is aimed toward explaining which components of the model affect probability of a purchase.

Functional form: In many cases, a **linear model** allows for a relatively interpretable form, but may not be as flexible or accurate as other models.

### 2.1.2 How Do We Estimate $f$?

There are many different approaches to estimating $f$, which all share certain characteristics and terms.

-   Training Data: This is the data used to train or teach our model how to estimate $\hat{f}$. In general, most estimation methods can be characterized as either **parametric** or **non-parametric**.

#### **Parametric Methods**:

Involves a two-step model-base approach:

1.  Assume functional form.

Example: $f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p$ (This is a *linear* model)

2.  After model selection, identify the procedure to estimate the parameters of the model. For linear models, this would be the method of estimating $\beta_0$, $\beta_1$, ... etc such that:

$Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p$

The most common approach with linear models is the **(Ordinary) least squares** method. The parametric method reduces estimation to determining a set of **parameters** that create the best fit for an assumed functional form.

Pros:

-   Assuming the form makes estimation simpler!

Potential Cons:

-   We don't know the true $f$, and we could be way off!

-   We can choose more flexible models to address this, but...

-   More flexible models lead to more parameters to estimate, and potentially **overfitting**.

#### **Non-parametric Methods**

Pro: Do not make assumptions about functional form.

Con: Require a large number of observations to obtain an estimate of $f$

### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability

```{r}
plot(0:10,
     type = 'n',
    xlim = c(0, 10),
    xaxt = 'none',
    ylim = c(0, 10),
    yaxt = 'none',
    xlab = "Flexibility", 
    ylab = "Interpretability")
axis(1, at = c(1, 8.75), labels = c("Low", "High"))
axis(2, at = c(1, 8.75), labels = c("Low", "High"))
text(x=1, y=9.5, "Subset Selection", font=1)
text(x=1, y=8.5, "Lasso", font=1)
text(x=3.25, y=6.75, "Least Squares", font=1)
text(x=4.75, y=5, "Generalized Additive Models", font=1)
text(x=4.75, y=4.5, "Trees", font=1)
text(x=8.75, y=3, "Bagging, Boosting", font=1)
text(x=7.5, y=1.25, "Support Vector Machines", font=1)

```

| Method             | Pro                | Con                     |
|--------------------|--------------------|-------------------------|
| Linear Regression  | Easy to interpret  | Relatively inflexible   |
| Thin Plate Splines | Very flexible      | Difficult to understand |
| **lasso**          | More interpretable | less flexible           |
| GAMs               | more flexible      | less interpretable      |

### 2.1.4 Supervised Versus Unsupervised Learning

> Most statistical learning problems fall into one of two categories: supervised or unsupervised.

*Supervised Learning*: For each observation of the predictor measurements $X_i$, there is an associated response measurement $Y_i$. These are models where we want to predict **outcomes**.

*Unsupervised Learning*: For each observation of the predictor measurements $X_i$, there is **No** associated response measurement $Y_i$(!) - In this scenario, it is not possible to fit a linear regression, since there is no associated $Y_i$.

#### Cluster Analysis

One way to understand unsupervised models is through **cluster analysis**. The goal of this type of analysis is to determine whether $x_i, ..., x_n$ fall into relatively distinct groups.

Note:

-   Clustering methods are imprecise -- They cannot assign all points to their correct group.

-   If there are $p$ variables, then $p(p- 1) /2)$ scatterplots can be made, this is why automated clustering methods are important.

-   There are instances where it is not clear whether a problem is *supervised* or *unsupervised* -- Some $Y$'s exist, but not all. These are referred to as *semi-supervised learning problems*.

```{r}
iris_cluster <- iris[, -5]
cls <- kmeans(x = iris_cluster, centers = 3)
iris_cluster$cluster <- as.character(cls$cluster)
ggplot() +
  geom_point(data = iris_cluster, 
             mapping = aes(x = Sepal.Length, 
                                  y = Petal.Length, 
                                  colour = cluster))
# borrowed from: https://rpubs.com/aephidayatuloh/clustervisual
```

### 2.1.5 Regression Versus Classification Problems

-   Problems with a *quantitative response value* (numeric) are referred to as *regression problems*.
-   Problems with a *qualitative response* -- a value in one of $K$ different classes, are referred to as *classification problems*.
-   Qualitative responses are also referred to as *categorical values*.

## 2.2 Assessing Model Accuracy

> There is no free lunch in statistics: no one method dominates all others over all possible data sets. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set.

### 2.2.1 Measuring the Quality of Fit

When using regressions, quality of fit is most commonly assessed by **mean squared error** (MSE):

$MSE = \frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(x_i))^2$

$\hat{f}(x_i))^2$ is the prediction.

The *training* MSE will be small if the predicted responses are close to the true responses, and larger if the estimates of the predictions are farther from the true responses.

Examples:

-   If we are interested in stock prices based on the previous 6 months, we really only care about how well the algorithm predicts *tomorrow's price*.

-   If we train a model on diabetes patient's clinical measurements, we are only concerned with how well the model predicts *future* diabetes patients.

Mechanically: If we fit our method on training observations ${(x_1, y_1), (x_2, y_2), …, (x_n, y_n)}$, we use those observations to fit $\hat{f}(x_1), \hat{f}(x_2), …, \hat{f}(x_n)$.

The aim here is to compute an $\hat{f}(x_0)$ which is closest to the real *unseen* $y_0$ observation, the test data.

#### How do we choose our model?

If we have test data available (not used for training/estimating $\hat{f}$), we can simply choose the method which minimizes $MSE$ on that test data. If we do not have testing data, we can choose the model which minimizes $MSE$ for our training data, but ***there is no guarantee that a method with the smallest training*** $MSE$ ***will result in the smallest test*** $MSE$***.***

Note: As model flexibility increases, the training $MSE$ will decrease, but this does not imply that the *test* $MSE$ will similarly decrease. When a method yields a small training $MSE$ and a large test $MSE$, we are *overfitting* our data.

### 2.2.2 The Bias-Variance Trade-Off

It is possible to prove that the expected test MSE can be decomposed into the sum of three quantities: the v*ariance* of $\hat{f}(x_0)$, the squared *bias* of $\hat{f}(x_0)$, and the variance of the error terms $\epsilon$.

$$
E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2 + Var(\epsilon)
$$

To achieve a low expected test error, it is necessary to select a method that results in *low variance* and *low bias*. It's also important to understand that the MSE will never be lower than the $Var(\epsilon)$, the irreducible error.

**Variance** refers to the amount $\hat{f}$ would change if we used different testing data. Generally, more flexible models have higher variance.

**Bias** is the error introduced by using a simple model to approximate potentially complex functions. More flexible models generally have less bias.

The Bias-Variance trade-off is the challenge of identifying a model which has both low variance **and** low bias.

### 2.2.3 The Classification Setting

In the classification context, many of the concepts above still apply, with minor differences because the $y_0$ is no longer a number value, but instead a qualitative value. The most common approach for gauging the accuracy of a qualitative $\hat{f}$ is the training *error rate:*

$$
\frac 1 n \sum_{i=1}^n I(y_i \neq \hat{y_i})
$$

Where $\hat{y_i}$ is the predicted value for the $i$th observation using the function $\hat{f}$,

$I(y_i \neq \hat{y_i})$ is an *indicator variable*, equal to 1 if $y_i \neq \hat{y_i}$ and zero if not. This computes the fraction of incorrect classifications.  

As with regression methods, the our aim should be to reduce the test error rate.  

#### The Bayes Classifier  

There is a special case in which it can be shown that the test error rate is minized by *assigning each observation to it's most likely class, based on it's predictor values*.  

This case is called the **Bayes Classifier**:  

$$
Pr(Y = j | X = x_0)
$$  
This is the *conditional probability* that $Y = j$, given the observed value $x_0$.  
  
  TBC


## 2.3 Lab: Introduction to R  

### 2.3.1 Basic Commands  

To run a function called `function`, we type `function(input)`. Objects are defined and then can be called by themselves.

```{r}
# create a vector of numbers with the c() function
x <- c(1,3,2,5)
x
print(x)
```

We can check the length of an object in R using the `length()` function.

```{r}
length(x) # 4
```

The `matrix()` function can be used to create matrices of any size. 
```{r}
x <- matrix(data=c(1,2,3,4),nrow=2,ncol=2)
x
```

The `sqrt()` function returns the square root of an object passed to it. 
```{r}
sqrt(x) # 4
```

The `rnorm()` function generates a vectors of random normal variables. We can use the `cor()` function to compute the correlation between two vectors.  
```{r}
x <- rnorm(50)
y = x * rnorm(5, mean = 50, sd = .1)
```


### 2.3.2 Graphics

`plot()` is the primary plotting function in base R. `plot(x,y)` will produce a plot with the vector `x` on the x-axis, and `y` on the y-axis.  
```{r}
plot(x, y)
```
Other useful functions:  

-    `mean()`
-    `var()`
-    `sqrt()`
-    `sd()`
-    `pdf()`
-    `jpeg()`
-    `dev.off()`
-    `seq()`

```{r}
x <- seq(1, 10)
y <- x
f <- outer(x, y, function(x, y) cos(y) / (1+x^2))
contour(x, y, f)
# contour(x, y, f, nlevels = 45, add = T)

```

### 2.3.3 Indexing Data  

Indexing is useful for inspecting specific parts of whatever data we are working with.

```{r}
A <- matrix(1:16,4,4)
A

```
To access the third element of the second column:

```{r}
A[2, 3] # Row 2, Column 3
```
We can also access multiple rows or columns of data at once: 
```{r}
A[c(2:4), 3] # Rows 2 through 4, in Column 3
```
### 2.3.4 Loading Data

To work with data in R, the first step is to load it into your session. The `read.table()` function can be used for this. There are lots of other functions you can use to read data into your session, including those from external packages.

```{r}
Auto <- read.table("data/Auto.data")
head(Auto)
```

#### Troubleshooting

It is a good idea to visually inspect your data before and after loading it into your R session. In this case, we have loaded the `Auto.data` incorrectly, and R assumes there are no column name values. To fix this:

```{r}
Auto <- read.table("data/Auto.data", 
                   # argument for a header
                   header = T, 
                   # convert "?" strings to NA
                   na.strings = "?")
```

Other useful functions:  
-    `na.omit()`
-    `dim()`

### 2.3.5 Additional Graphical and Numerical Summaries

We can use the `plot()` function to create *scatterplots* of quantitative variables. When using this function, it is necessary to specify the dataset name: 

```{r}
plot(x = as.factor(Auto$cylinders), y = Auto$mpg)
```

In the graph above, `cylinder` is converted in a factor variable, since there are only a specific number of possible values. If the variable on the $x$-axis is categorical, *boxplots* will automatically be drawn on the plot.

```{r}
# There are many plot options available 
hist(Auto$mpg, col = "purple", 
     breaks = 15, 
     main = "Histogram of Miles per Gallon", xlab = "MPG")
```

## 2.4 Exercises

## Conceptual

1.  For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.  
(a)    The sample size $n$ is extremely large, and the number of predictors $p$ is small.  
>  A flexible model would benefit from the large sample and would fit the data better.

(b)    The number of predictors $p$ is extremely large, and the number of observations $n$ is small.  
>  A flexible model would perform worse here and overfit because of the small $n$. 

(c)    The relationship between the predictors and response is highly non-linear.  
>  In this case, a more flexible model would perform better than an inflexible one.  

(d)    The variance of the error terms, i.e. $\sigma^2 = Var(\epsilon)$, is extremely high.
>  A flexible method would do worse in this situation, because it would fit to the noise in the error terms.  

2.  Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$.  

(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.  
>  Prediction. $n$ is 500, $p$ is 3 -- profit, employees, and industry. 

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.  
>  Classification, the outcome variable will be either be 'success' or a 'failure'. $n$ is 20. $p$ is 13 -- price, marketing budget, competition price, and the 10 other variables.  

3.  We now revisit the bias-variance decomposition.  

(a)  Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves.Make sure to label each one.  

```{r}

```

(b)  Explain why each of the five curves has the shape displayed in part (a).  

4.  You will now think of some real-life applications for statistical learning.  

(a)  Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.  

(b)  Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.  

c)  Describe three real-life applications in which cluster analysis might be useful.  

5.  What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?  

6.  Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?  

7.  The table below provides a training data set containing six observations, three predictors, and one qualitative response variable. 

| $Obs$|$X_1$ |$X_2$|$X_3$|$Y$|
|---|---|---|---|---|
|$1$ |$0$  |$3$ |$0$ |$Red$|
|$2$ |$2$  |$0$ |$0$ |$Red$|
|$3$ |$0$  |$1$ |$3$ |$Red$|
|$4$ |$0$  |$1$ |$2$ |$Green$|
|$5$ |$-1$ |$0$ |$1$ |$Green$|
|$6$ |$1$ | $1$ |$1$ |$Red$|

Suppose we wish to use this data set to make a prediction for Y when $X_1 = X_2 = X_3 = 0$ using $K$-nearest neighbors.

(a) Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$.  

(b) What is our prediction with $K = 1$? Why?  
(c) What is our prediction with $K = 3$? Why?  
(d) If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for $K$ to be large or small? Why?  

## Applied

8.  This exercise relates to the `College` data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are  

-    `Private`: Public/private indicator
-    `Apps`: Number of applications received
-    `Accept`: Number of applicants accepted
-    `Enroll`: Number of new students enrolled
-    `Top10perc`: New students from top 10% of high school class
-    `Top25perc`: New students from top 25% of high school class
-    `F.Undergrad`: Number of full-time undergraduates
-    `P.Undergrad`: Number of part-time undergraduates
-    `Outstate`: Out-of-state tuition
-    `Room.Board`: Room and board costs
-    `Books`: Estimated book costs
-    `Personal`: Estimated personal spending
-    `PhD`:Percent of faculty with Ph.D.’s
-    `Terminal`:Percent of faculty with terminal degree
-    `S.F.Ratio`: Student/faculty ratio
-    `perc.alumni`: Percent of alumni who donate
-    `Expend`: Instructional expenditure per student
-    `Grad.Rate`: Graduation rate  

Before reading the data into R, it can be viewed in Excel or a text editor.  

(a) Use the `read.csv()` function to read the data into R. Call the loaded data `college`. Make sure that you have the directory set to the correct location for the data.  
```{r}
if(!file.exists("data/Collage.csv")){
  download.file("https://www.statlearning.com/s/College.csv", destfile = "data/College.csv")
  }
college <- read.csv("data/College.csv")
```

(b) Look at the data using the fix() function. You should notice that the first column is just the name of each university. We don't really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:
```{r}
# fix(college)
# add first column as rownames
rownames(college) <- college[, 1]
college <- college[, -1]
```

(c)    i. Use the `summary()` function to produce a numerical summary of the variables in the data set.
```{r}
summary(college)
```

(c)    ii. Use the `pairs()` function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix `A` using `A[,1:10]`.
```{r}
pairs(college[, 1:10])
```

(c)    iii. Use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Private`.
```{r}
plot(as.factor(college$Private), college$Outstate, xlab = "Private", ylab = "Out-of-state Tuition")
```

(c)    v. Create a new qualitative variable, called `Elite`, by binning the `Top10perc` variable. We are going to divide universitiesinto two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.  
```{r}
Elite <- rep("No",nrow(college))
Elite[college$Top10perc>50]="Yes" 
Elite <- as.factor(Elite)
college=data.frame(college,Elite)
```

Use the `summary()` function to see how many elite universities there are. Now use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Elite`.
```{r}
summary(college$Elite)
plot(as.factor(college$Elite), college$Outstate, xlab = "Elite", ylab = "Out-of-state Tuition")
```

(c)    v. Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow=c(2,2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.
```{r}
par(mfrow=c(2,2))
hist(college$Top10perc)
hist(college$F.Undergrad, breaks = 15)
hist(college$S.F.Ratio, breaks = 5)
hist(college$Room.Board, breaks = 10)
```

(c)    vi. Continue exploring the data, and provide a brief summary of what you discover.  
```{r}
plot(college$S.F.Ratio, college$perc.alumni,
     xlab = "Student/Faculty Ratio", 
     ylab = "Percent of Alumni who Donate"
     )
lines(predict(lm(perc.alumni~S.F.Ratio, data = college)),
      col='red')
```
> Plotting Student/Faculty ratio and the percent of alumni who donate does not show clear relationship that I thought would show up. etc etc discuss.  

9.  This exercise involves the `Auto` data set studied in the lab. Make sure that the missing values have been removed from the data.  
```{r}
Auto <- read.table("data/Auto.data", header = T, na.strings = "?")
Auto <- na.omit(Auto)
```





