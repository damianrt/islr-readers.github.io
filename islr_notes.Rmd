--- 
title: "ISLR Notes"
author: "TBD"
date: "2021"
site: bookdown::bookdown_site
documentclass: book
#bibliography: [book.bib, packages.bib]
#biblio-style: apalike
link-citations: yes
---

# About {-#about}

Notes and solutions for the exercises in the book: *An Introduction to Statistical Learning with Applications in R (1st edition)* by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (website:  https://www.statlearning.com/)

**License**

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>

This work, as a whole, is licensed under a [Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/) 

<!--chapter:end:index.Rmd-->

---
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction {#intro}

```{r include = FALSE}
# Options
knitr::opts_chunk$set(echo = FALSE)

# Packages
library(tidyverse)
library(ISLR)

# Get raw data from the R package
data(Wage, Smarket, NCI60, package = "ISLR")
```

## An Overview of Statistical Learning

> "Statistical learning refers to a vast set of tools for understanding
> data."

-   Supervised: Using statistical models to **predict** or **estimate**
    **outputs** based on **inputs**.
-   Unsupervised: Finding relationships between variables and structure
    in the data

## Data sets

Example data used in the book

-   Wages
-   Stock Market Data
-   Gene Expression Data

### Wages

Used for regression problem examples such as predicting wage based on
age and education

```{r echo = TRUE}
glimpse(Wage)
```

```{r }

Wage %>%
    ggplot(mapping = aes(x = age, y = wage)) +
    geom_point(alpha = 0.25) +
    geom_smooth(formula = y~x, method = "loess", se = FALSE)
    
Wage %>%
    ggplot(mapping = aes(x = year, y = wage)) +
    geom_point(alpha = 0.25) +
    geom_smooth(formula = y~x, method = "loess", se = FALSE)
    
Wage %>%
    ggplot(mapping = aes(x = education, y = wage, 
                         fill = education)) +
    geom_boxplot()
```

### Stock Market Data

Used for classification problem examples with categorical or qualitative
output, such as predicting whether a stock index will either increase or
decrease on any given day.

Daily percentage change of S&P 500 stock index and 5 prior days

```{r echo = TRUE}
glimpse(Smarket)
```

```{r }
Smarket %>%
    select(Direction, Today, Lag1, Lag2, Lag3) %>%
    pivot_longer(cols = c("Lag1", "Lag2", "Lag3"), names_to = "Lag", values_to = "Returns") %>%
    mutate(
        Lag = factor(Lag, levels = c("Lag1", "Lag2", "Lag3"), 
                     labels = c("One Day", "Two Days", "Three Days"), 
                     ordered = TRUE)
    ) %>%
    ggplot(mapping = aes(x = Direction, y = Returns, fill = Direction)) +
    geom_boxplot() + 
    scale_fill_manual(values = c("Down" = "steelblue2", "Up" = "sienna2")) +
    facet_wrap(. ~ Lag) +
    labs(y = "Percentage change in Stock Index",
         x = "Today's Direction")
```

### Gene Expression Data

Used for examples of clustering problems such as identifying related
groups of cancer cells based on observed characteristics.

```{r echo = TRUE}
str(NCI60)
```

## History

A brief timeline for the development of statistical learning

-   1800's *Linear Regression* (*Method of Least Squares*)
-   1936 *Linear Discriminant Analysis* developed to predict qualitative
    values
-   1940s *Logistic Regression* developed to predict qualitative values
-   1970s *Generalized Linear Models* including both logistic and linear
    regression
-   1980s *Classification and Regression Trees*
-   1986 *Generalized Additive Models*
-   Present day (2001) *Machine Learning*

## Other Considerations

["How Eugenics Shaped Statistics: Exposing the damned lies of three
science
pioneers.](https://nautil.us/issue/92/frontiers/how-eugenics-shaped-statistics)

## Matrix Notation

Conventions used in the book

-   $n$ number of observations in a sample

-   $p$ number of variables

-   $\textbf{X}$ an $n \times p$ matrix

    -   where $x_{ij}$ represents the element in the $i$th row and the
        $j$th column.
    -   $x_i$ represents a single observation (row) as a vector with
        length $p$. Note that vectors are written vertically by
        convention in math notation.
    -   $\textbf{x}_j$ represents a single variable (column) as a vector
        with length $n$. Note that the bold face font is used to
        distinguish columns ($\textbf{x}_3$) from rows ($x_3$).

-   The $^T$ superscript operator denotes the transpose of a matrix or
    vector, where row and column indices are reversed such that the
    resulting matrix or vector will have $p$ rows and/or $n$ columns.

Examples

-   A matrix of elements $$
    \textbf{X} =  \left(
    \begin{matrix} 
    x_{11} & x_{12} & \dots  & x_{1p} \\
    x_{21} & x_{22} & \dots  & x_{2p} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & \dots  & x_{np} 
    \end{matrix}
    \right)
    $$

-   A row vector $$
    x_i = \left(\begin{matrix} x_{i1} \\ x_{i2} \\ \vdots \\ x_{ip} \end{matrix} \right)
    $$

-   A column vector $$
    \textbf{x}_j = \left(\begin{matrix} x_{1j} \\ x_{2j} \\ \vdots \\ x_{nj} \end{matrix} \right)
    $$

-   A matrix represented as a collection of column vectors $$
    \textbf{X} =  \left( \textbf{x}_1, \textbf{x}_2, \dots, \textbf{x}_j \right)
    $$

-   A transposed matrix. Rows become columns and columns become rows $$
    \textbf{X}^{T} =  \left(
    \begin{matrix} 
    x_{11} & x_{12} & \dots  & x_{1n} \\
    x_{21} & x_{22} & \dots  & x_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{p1} & x_{p2} & \dots  & x_{pn} 
    \end{matrix}
    \right)
    $$

-   A transposed row vector. Again, vector elements are listed
    vertically by default, so this presentation shows the new
    orientation. $$
    x_{i}^{T} =  \left( x_{i1}, x_{i2}, \dots, x_{ip} \right)
    $$

-   A matrix represented as a collection of row vectors $$
    \textbf{X} =  \left(
    \begin{matrix} 
    x_{1}^T \\
    x_{2}^T \\
    \vdots \\
    x_{n}^T
    \end{matrix}
    \right)
    $$

<!--chapter:end:01-introduction.Rmd-->

# Statistical Learning

```{r include = FALSE}
# Options
knitr::opts_chunk$set(echo = FALSE)

# Packages
library(tidyverse)
library(ISLR)

# get Advertising data from ISLR Site
if(!file.exists("data/Advertising.csv")){
    download.file("https://www.statlearning.com/s/Advertising.csv", 
                  destfile = "data/Advertising.csv")
    Advertising <- read_csv("data/Advertising.csv") %>% select(-X1)
} 
Advertising <- read_csv("data/Advertising.csv") %>% select(-X1)

if(!file.exists("data/Income1.csv")){
    download.file("https://www.statlearning.com/s/Income1.csv", 
                  destfile = "data/Income1.csv")
    Income1 <- read_csv("data/Income1.csv") %>% select(-X1)
} 
Income1 <- read_csv("data/Income1.csv")
```

## 2.1 What Is Statistical Learning?

Motivating example: 
> Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. ... our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.

```{r}
glimpse(Advertising)
```

```{r}
Advertising %>%
    ggplot(mapping = aes(x = TV, y = sales)) +
    geom_point(alpha = 0.25, shape = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)
Advertising %>%
    ggplot(mapping = aes(x = radio, y = sales)) +
    geom_point(alpha = 0.25, shape = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)
Advertising %>%
    ggplot(mapping = aes(x = newspaper, y = sales)) +
    geom_point(alpha = 0.25, shape = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)

```

**Input Variables**: These are the variables we knoW and can use to build our model. Also known as *predictors*, *independent variables*, or *features*. Denoted using the symbol $X_n$.

**Output Variable**: This is the variable we are trying to predict with the model. Also known as a *response*, or *dependent variable*. Typically denoted as $Y$.

More generally: $Y = f(X) + \epsilon$

Where $Y$ is the quantitative response and $f$ is a function of $X_1, ..., X_p$ (of $p$ different predictors) and $\epsilon$ is some random **error term**.

Assumptions:

-   $f$ is **systematic** in its relationship to $Y$
-   $\epsilon$ is independent of $X$
-   $\epsilon$ has mean zero

Another example: Income and education may appear related, but the exact relationship is unknown. Note that some of the observations are above the linear interpolated line, while some are below it. The difference is $\epsilon$

```{r}
Income1 %>%
    ggplot(mapping = aes(x = Education, y = Income)) +
    geom_point(color = "red") + 
        geom_smooth(formula = y~x, method = "lm", se = FALSE) +
    theme_bw() 

```

## 2.1.1 Why Estimate f?

There are two main reasons to estimate $f$:
-   Prediction 
-   Inference  

### Prediction

Consider: $\hat{Y} = \hat{f}(X)$

If $X$ is known, we can predict $\hat{Y}$ by this equation. Don't be too concerned with the exact functional form of $\hat{f}$, as long as it yields accurate predictions of $Y$.

The accuracy of $\hat{Y}$ depends on two quantities:

-   **Reducible error**: This is error that comes with the model. We can potentially address this error by improving the accuracy of the model.

-   **Irreducible error**: This is error introduced to the model, because $\epsilon$, by definition, cannot be explained by $X$

**Why is irreducible error larger than zero?** Consider the estimate $\hat{f}$ and a prediction $\hat{Y} = \hat{f}(X)$. Let $\hat{f}$ and $X$ be fixed. Then:

$E(Y - Y^2) = E[f(X) + \epsilon - \hat{f}(X)]^2$

$= [f(X) - \hat{f}(X)]^2 + Var(\epsilon)$

Where $E(Y - Y^2)$ is the **expected value** of the squared difference between the predicted and actual value of $Y$, and $Var(X)$ is the **variance** associated with the error term $\epsilon$. 

#### Inference:

When used for inference, the aim is not to use estimate $f$ for predictions, but rather to understand how some response $Y$ is affected by the changes in $X_1, ..., X_p$. 

-   **Which predictors are associated with the response?**: Identifying the **important** predictors is the aim here.   
-   **What is the relationship between the response and each predictor?**: This can be positive, negative, or depend on the values of other predictors, depending on how complicated the model is.  
-   **Can the relationship between $Y$ and each predictor be summarized using a linear equation?**

Examples:
-   **Prediction**: A Company using a model to identify target customers for a direct-marketing campaign. The company is not interested in the model, they just want a function form that will help them.
-   **Inference**: Modeling customer purchases of specific brands of products. The model is aimed toward explaining which components of the model affect probability of a purchase.

Functional form: In many cases, a **linear model** allows for a relatively interpretable form, but may not be as flexible or accurate as other models. 

## 2.1.2 How Do We Estimate $f$?

There are many different approaches to estimating $f$, which all share certain characteristics and terms. 

-   Training Data: This is the data used to train or teach our model how to estimate $\hat{f}$. In general, most estimation methods can be characterized as either **parametric** or **non-parametric**. 

### **Parametric Methods**: 

Involves a two-step model-base approach: 

1. Assume functional form. 

Example: $f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p$  (This is a *linear* model)

2. After model selection, identify the procedure to estimate the parameters of the model. For linear models, this would be the method of estimating $\beta_0$, $\beta_1$, ... etc such that: 

$ Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p$ 

The most common approach with linear models is the **(Ordinary) least squares** method. The parametric method reduces estimation to determining a set of **parameters** that create the best fit for an assumed functional form. 

Pros:  
-   Assuming the form makes estimation simpler!

Potential Cons:  
-   We don't know the true $f$, and we could be way off!
-   We can choose more flexible models to address this, but...
-   More flexible models lead to more parameters to estimate, and potentially **overfitting**.

### **Non-parametric Methods**

Pro: Do not make assumptions about functional form.
Con: Require a large number of observations to obtain an estimate of $f$

## 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability

```{r}
plot(0:10,
     type = 'n',
    xlim = c(0, 10),
    xaxt = 'none',
    ylim = c(0, 10),
    yaxt = 'none',
    xlab = "Flexibility", 
    ylab = "Interpretability")
axis(1, at = c(1, 8.75), labels = c("Low", "High"))
axis(2, at = c(1, 8.75), labels = c("Low", "High"))
text(x=1, y=9.5, "Subset Selection", font=1)
text(x=1, y=8.5, "Lasso", font=1)
text(x=3.25, y=6.75, "Least Squares", font=1)
text(x=4.75, y=5, "Generalized Additive Models", font=1)
text(x=4.75, y=4.5, "Trees", font=1)
text(x=8.75, y=3, "Bagging, Boosting", font=1)
text(x=7.5, y=1.25, "Support Vector Machines", font=1)

```

## 2.1.4 Supervised Versus Unsupervised Learning

## 2.1.5 Regression Versus Classification Problems

## 2.2 Assessing Model Accuracy

## 2.2.1 Measuring the Quality of Fit

## 2.2.2 The Bias-Variance Trade-Off

## 2.2.3 The Classification Setting

## 2.3 Lab: Introduction to R

## 2.3.1 Basic Commands

```{r}
#  x <- c(1,3,2,5)
# x
```

## 2.3.2 Graphics

## 2.3.3 Indexing Data

## 2.3.4 Loading Data

## 2.3.5 Additional Graphical and Numerical Summaries

## 2.4 Exercises

## Conceptual

1.  

2.  

3.  

4.  

5.  

6.  

7.  

## Applied

8.  

9.  

10. 

<!--chapter:end:02-statistical-learning.Rmd-->

# Linear Regression


<!--chapter:end:03-linear-regression.Rmd-->

# Classification 


<!--chapter:end:04-classificiation.Rmd-->

# Resampling Methods


<!--chapter:end:05-resampling-methods.Rmd-->

# Model Selection and Regularization


<!--chapter:end:06-model-selection.Rmd-->

# Moving Beyond Linearity

<!--chapter:end:07-beyond-linearity.Rmd-->

# Tree Based Methods

<!--chapter:end:08-tree-based-methods.Rmd-->

# Support Vector Machines


<!--chapter:end:09-support-vector-machines.Rmd-->

# Unsupervised Learning


<!--chapter:end:10-unsupervised-learning.Rmd-->

