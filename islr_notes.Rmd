--- 
title: "ISLR Notes"
author: "TBD"
date: "2021"
site: bookdown::bookdown_site
documentclass: book
#bibliography: [book.bib, packages.bib]
#biblio-style: apalike
link-citations: yes
---
--- 
title: "ISLR Notes"
author: "TBD"
date: "2021"
site: bookdown::bookdown_site
documentclass: book
#bibliography: [book.bib, packages.bib]
#biblio-style: apalike
link-citations: yes
---

# About {-#about}

Notes and solutions for the exercises in the book: *An Introduction to Statistical Learning with Applications in R (1st edition)* by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (website:  https://www.statlearning.com/)

**License**

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>

This work, as a whole, is licensed under a [Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/) 

<!--chapter:end:index.Rmd-->


# Introduction {#intro}

Placeholder


## An Overview of Statistical Learning
## Data sets
### Wages
### Stock Market Data
### Gene Expression Data
## History
## Other Considerations
## Matrix Notation

<!--chapter:end:01-introduction.Rmd-->

# Statistical Learning

```{r include = FALSE}
# Options
knitr::opts_chunk$set(echo = FALSE)

# Packages
library(tidyverse)
library(ISLR)

# get Advertising data from ISLR Site
if(!file.exists("data/Advertising.csv")){
    download.file("https://www.statlearning.com/s/Advertising.csv", 
                  destfile = "data/Advertising.csv")
    Advertising <- read_csv("data/Advertising.csv") %>% select(-X1)
} 
Advertising <- read_csv("data/Advertising.csv") %>% select(-X1)

if(!file.exists("data/Income1.csv")){
    download.file("https://www.statlearning.com/s/Income1.csv", 
                  destfile = "data/Income1.csv")
    Income1 <- read_csv("data/Income1.csv") %>% select(-X1)
} 
Income1 <- read_csv("data/Income1.csv")
```

## 2.1 What Is Statistical Learning?

Motivating example: 
> Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. ... our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.

```{r}
glimpse(Advertising)
```

```{r}
Advertising %>%
    ggplot(mapping = aes(x = TV, y = sales)) +
    geom_point(alpha = 0.25, point = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)
Advertising %>%
    ggplot(mapping = aes(x = radio, y = sales)) +
    geom_point(alpha = 0.25, point = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)
Advertising %>%
    ggplot(mapping = aes(x = newspaper, y = sales)) +
    geom_point(alpha = 0.25, point = 1) + 
    theme_bw() +
    geom_smooth(formula = y~x, method = "lm", se = FALSE)

```

**Input Variables**: These are the variables we knoW and can use to build our model. Also known as *predictors*, *independent variables*, or *features*. Denoted using the symbol $X_n$.

**Output Variable**: This is the variable we are trying to predict with the model. Also known as a *response*, or *dependent variable*. Typically denoted as $Y$.

More generally: $Y = f(X) + \epsilon$

Where $Y$ is the quantitative response and $f$ is a function of $X_1, ..., X_p$ (of $p$ different predictors) and $\epsilon$ is some random **error term**.

Assumptions:

-   $f$ is **systematic** in its relationship to $Y$
-   $\epsilon$ is independent of $X$
-   $\epsilon$ has mean zero

Another example: Income and education may appear related, but the exact relationship is unknown. Note that some of the observations are above the linear interpolated line, while some are below it. The difference is $\epsilon$

```{r}
Income1 %>%
    ggplot(mapping = aes(x = Education, y = Income)) +
    geom_point(point = 1, color = "red") + 
        geom_smooth(formula = y~x, method = "lm", se = FALSE) +
    theme_bw() 

```

## 2.1.1 Why Estimate f?

There are two main reasons to estimate $f$:
-   Prediction 
-   Inference  

### Prediction

Consider: $\hat{Y} = \hat{f}(X)$

If $X$ is known, we can predict $\hat{Y}$ by this equation. Don't be too concerned with the exact functional form of $\hat{f}$, as long as it yields accurate predictions of $Y$.

The accuracy of $\hat{Y}$ depends on two quantities:

-   **Reducible error**: This is error that comes with the model. We can potentially address this error by improving the accuracy of the model.

-   **Irreducible error**: This is error introduced to the model, because $\epsilon$, by definition, cannot be explained by $X$

**Why is irreducible error larger than zero?** Consider the estimate $\hat{f}$ and a prediction $\hat{Y} = \hat{f}(X)$. Let $\hat{f}$ and $X$ be fixed. Then:

$E(Y - Y^2) = E[f(X) + \epsilon - \hat{f}(X)]^2$

$= [f(X) - \hat{f}(X)]^2 + Var(\epsilon)$

Where $E(Y - Y^2)$ is the **expected value** of the squared difference between the predicted and actual value of $Y$, and $Var(X)$ is the **variance** associated with the error term $\epsilon$. 

#### Inference:

When used for inference, the aim is not to use estimate $f$ for predictions, but rather to understand how some response $Y$ is affected by the changes in $X_1, ..., X_p$. 

-   **Which predictors are associated with the response?**: Identifying the **important** predictors is the aim here.   
-   **What is the relationship between the response and each predictor?**: This can be positive, negative, or depend on the values of other predictors, depending on how complicated the model is.  
-   **Can the relationship between $Y$ and each predictor be summarized using a linear equation?**

Examples:
-   **Prediction**: A Company using a model to identify target customers for a direct-marketing campaign. The company is not interested in the model, they just want a function form that will help them.
-   **Inference**: Modeling customer purchases of specific brands of products. The model is aimed toward explaining which components of the model affect probability of a purchase.

Functional form: In many cases, a **linear model** allows for a relatively interpretable form, but may not be as flexible or accurate as other models. 

## 2.1.2 How Do We Estimate $f$?

There are many different approaches to estimating $f$, which all share certain characteristics and terms. 

-   Training Data: This is the data used to train or teach our model how to estimate $\hat{f}$. In general, most estimation methods can be characterized as either **parametric** or **non-parametric**. 

### **Parametric Methods**: 

Involves a two-step model-base approach: 

1. Assume functional form. 

Example: $f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p$  (This is a *linear* model)

2. After model selection, identify the procedure to estimate the parameters of the model. For linear models, this would be the method of estimating $\beta_0$, $\beta_1$, ... etc such that: 

$ Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p$ 

The most common approach with linear models is the **(Ordinary) least squares** method. The parametric method reduces estimation to determining a set of **parameters** that create the best fit for an assumed functional form. 

Pros:  
-   Assuming the form makes estimation simpler!

Potential Cons:  
-   We don't know the true $f$, and we could be way off!
-   We can choose more flexible models to address this, but...
-   More flexible models lead to more parameters to estimate, and potentially **overfitting**.

### **Non-parametric Methods**

Pro: Do not make assumptions about functional form.
Con: Require a large number of observations to obtain an estimate of $f$

## 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability

```{r}
plot(0:10,
     type = 'n',
    xlim = c(0, 10),
    xaxt = 'none',
    ylim = c(0, 10),
    yaxt = 'none',
    xlab = "Flexibility", 
    ylab = "Interpretability")
axis(1, at = c(1, 8.75), labels = c("Low", "High"))
axis(2, at = c(1, 8.75), labels = c("Low", "High"))
text(x=1, y=9.5, "Subset Selection", font=1)
text(x=1, y=8.5, "Lasso", font=1)
text(x=3.25, y=6.75, "Least Squares", font=1)
text(x=4.75, y=5, "Generalized Additive Models", font=1)
text(x=4.75, y=4.5, "Trees", font=1)
text(x=8.75, y=3, "Bagging, Boosting", font=1)
text(x=7.5, y=1.25, "Support Vector Machines", font=1)

```

## 2.1.4 Supervised Versus Unsupervised Learning

## 2.1.5 Regression Versus Classification Problems

## 2.2 Assessing Model Accuracy

## 2.2.1 Measuring the Quality of Fit

## 2.2.2 The Bias-Variance Trade-Off

## 2.2.3 The Classification Setting

## 2.3 Lab: Introduction to R

## 2.3.1 Basic Commands

```{r}
#  x <- c(1,3,2,5)
# x
```

## 2.3.2 Graphics

## 2.3.3 Indexing Data

## 2.3.4 Loading Data

## 2.3.5 Additional Graphical and Numerical Summaries

## 2.4 Exercises

## Conceptual

1.  

2.  

3.  

4.  

5.  

6.  

7.  

## Applied

8.  

9.  

10. 

<!--chapter:end:02-statistical-learning.Rmd-->

# Linear Regression


<!--chapter:end:03-linear-regression.Rmd-->

# Classification 


<!--chapter:end:04-classificiation.Rmd-->

# Resampling Methods


<!--chapter:end:05-resampling-methods.Rmd-->

# Model Selection and Regularization


<!--chapter:end:06-model-selection.Rmd-->

# Moving Beyond Linearity

<!--chapter:end:07-beyond-linearity.Rmd-->

# Tree Based Methods

<!--chapter:end:08-tree-based-methods.Rmd-->

# Support Vector Machines


<!--chapter:end:09-support-vector-machines.Rmd-->

# Unsupervised Learning


<!--chapter:end:10-unsupervised-learning.Rmd-->

